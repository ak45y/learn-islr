---
title: "chapter-6-exercises"
format: html
editor: source
tag: rstats/lasso rstats/ridge
---

# Exercises

### Exercise 1

- 1a. Best subset selection as the lowest training RSS as it fits models for every possible combination of
predictors. When p is very large, this increases the chance of finding models that fit the training data
very well.

- 1b. Best test RSS could be provided by any of the models. Bet subset considers more models than the
other two, and the best model found on the training set could also be the best one for a test set.
Forward and backward stepwise consider a lot fewer models, but might find a model that fits the test
set very well as it tends to avoid over fitting when compared to best subset.

- 1c. 

  - True; the k+1 model is derived by adding a predictor that gives greatest improvement to previous k
model.
  - True; the k model is derived by removing the least useful predictor from the k+1 model.
  - False; forward and backward stepwise can select different predictors.
  - False; forward and backward stepwise can select different predictors.

### Exercise 2

- 2a. (iii) Less flexible and will give improved prediction accuracy when its increase in bias is less than its decrease
in variance. As lambda increases, flexibility of fit decreases, and so the estimated coefficients decrease
with some being zero. This leads to a substantial decrease in the variance of the predictions for a small
increase in bias.

- 2b. (iii) - same concept as (a)

- 2c. (ii) Non-linear models will generally be more flexible, and so predictions tend to have a higher variance and
lower bias. So predictions will improve if the variance rises less than a decrease in the bias (bias-variance
trade off).


### Exercise 3

- 3a. (i) Decrease steadily. As s increases the constraint on beta decreases and the RSS reduces until we
reach the least squares answer

- 3b. (ii) Decreases initially as RSS in reduced from the maximum value (when B=0) as we move towards
the best training value for B1. Eventually starts increasing as the B values start fitting the training set
extremely well, and so over fitting the test set.

- 3c. (iii) Increases steadily

- 3d. (iv) Decreases steadily

- 3e. (v) Remains constant

### Exercsie 4

- 4a.  (iii) Steadily increase. As lambda increases the constraint on beta also increases, this means beta
becomes progressively smaller. As such the training RSS will steadily increase to the maximum
value (when B ~ 0 at very large lambda values.)

- 4b. (ii) Decreases initially and then starts increasing in a U shape.When lambda=0, we get a least squares
fit that has high variance and low bias. As lambda increases, the flexibility of the fit decreases,
so reducing variance of predictions for a small increase in bias. This results in more accurate
predictions and so the test RSS will decrease initially. As lambda increases beyond the ideal
point, we start seeing a much greater increase in bias than the reduction in variance, and so
predictions will become more biased. Consequently, we will see a rise in the test RSS.


- 4c. (iv) Steadily decrease

- 4d. (iii) Steadily increase

- 4e. (iv) Remains constant

### Exercsie 6

Eq - 6.12
$$\Sigma_{j=1}^p(y_j - \beta_j)^2 + \lambda\Sigma_{j=1}^p\beta_j^2$$


Eq - 6.14

$$\hat\beta_j^R = \frac{y_j}{(1+\lambda)}$$

```{r}
# Chart of Ridge Regression Eqn(6.12) against beta.
y = 10
beta = seq(-10,10,0.1)
lambda = 5
# Eqn (6.12) output
eqn1 = (y - beta)^2 + lambda*(beta^2)
which.min(eqn1) #index at minimum, similar to using (6.14).
```

```{r}
# Minimum beta estimated using (6.14).
est.beta = y/(1 + lambda)
est.value = (y - est.beta)^2 + lambda*(est.beta^2)
# Plot
plot(beta, eqn1, main="Ridge Regression Optimization", xlab="beta", ylab="Ridge Eqn output",type="l")
points(beta[118],eqn1[118],col="red", pch=24,type = "p")
points(est.beta, est.value,col="blue",pch=20,type ="p")

```

- For y=10 and lambda=5, beta=10/(1 + 5)= 1.67 minimizes the ridge regression equation.
- As can also be seen on the graph the minimum beta is at 1.67.

Eq - 6.13
$$\Sigma_{j=1}^p(y_j - \beta_j)^2 + \lambda\Sigma_{j=1}^p|\beta_j|$$


Eq - 6.15

![Equation 6.15](images/eq06-15.png)


```{r}
# Chart of Lasso Eqn (6.13) against beta.
# Eqn (6.13) output
eqn2 = (y - beta)^2 + lambda*(abs(beta))
# Minimum beta estimated using (6.15).
est.beta2 = y - lambda/2
est.value2 = (y - est.beta2)^2 + lambda*(abs(est.beta2))
# Plot
plot(beta, eqn2, main="Lasso Optimization", xlab="beta", ylab="Eqn 6.13 output",type="l")
points(est.beta2, est.value2,col="red",pch=20,type ="p")
```


- For y=10 and lambda=5, beta=10-(5/2)= 7.5 minimizes the lasso 
- As can also be seen on the graph the minimum beta is at 7.5.

### Exercise 7

We take our model as :

$$ y_i = \beta_0 + \sum_{j=1}^{p} x_{ij} \beta_j + \epsilon_i $$

Here the $\epsilon_i$ are IID from a normal random distrubtion
$N(0,\sigma^2)$ The likelihood is simply a product of normal
distributions with mean
$\mu_i = \beta_0 + \sum_{j=1}^{p} x_{ij} \beta_j$ and standard deviation
$\sigma$ :

$$ L \propto e^{-\frac{1}{2\sigma^2}\sum_i{(y_i - (\beta_0 + \sum_{j=1}^{p} x_{ij} \beta_j))^2} }$$
we only care about the parts that depends on the $\beta_i$ so dont worry
about the normalization.

The posterior is simply proportional to the product of $L$ and the prior

$$ P(\beta | Data) \propto P(Data | \beta) P(\beta)$$

$$ P(\beta | Data) \propto e^{-\frac{1}{2\sigma^2}\sum_i{(y_i - \beta_0 - \sum_{j=1}^{p} x_{ij} \beta_j)^2} } \prod_{j=1}^{p}e^{-\vert \beta_i \rvert/b}$$
again dropping any constants of proportionality that do not depend on
the parameters.

Now combine the exponentials:

$$ P(\beta | Data) \propto e^{-\frac{1}{2\sigma^2}\sum_i{(y_i - \beta_0 - \sum_{j=1}^{p} x_{ij} \beta_j)^2}  -\sum_{j=1}^{p}\vert \beta_i \rvert/b}$$

The mode of this distribution is the value for the $\beta_i$ for which
the exponent is maximized, which means to find the mode we need to
minimize:

$$ \frac{1}{2\sigma^2}\sum_i{(y_i - \beta_0 - \sum_{j=1}^{p} x_{ij} \beta_j)^2}  + \sum_{j=1}^{p}\vert \beta_i \rvert/b$$
or after multiplying through by $2 \sigma^2$

$$ \sum_i{(y_i - \beta_0 - \sum_{j=1}^{p} x_{ij} \beta_j)^2}  + \sum_{j=1}^{p} 2\sigma^2 \vert  \beta_i \rvert/b$$

This is the same form as 6.7

I think it should be clear that if you work throuhg the exact same steps
with prior (for each $\beta_i$) $e^{-\frac{\beta_i^2}{2 c}}$ you end up
with the posterior:

$$ e^{- \frac{1}{2\sigma^2}\sum_i{(y_i - \beta_0 - \sum_{j=1}^{p} x_{ij} \beta_j)^2}  -\sum_{j=1}^{p} \frac{\beta_i^2}{2 c}}$$

And to find the to find the mode of the posterior, to finding the
minimum of:

$$ \sum_i{(y_i - \beta_0 - \sum_{j=1}^{p} x_{ij} \beta_j)^2}  + \sum_{j=1}^{p} \frac{\sigma^2}{c}\beta_i^2$$
which is of the same form as 6.5. That this mode is also the mean
follows since the posterior in this case is a multinormal distribution
in $\beta_i$ (it's quadratic)
